# NGO Automation System - MVP
## FastAPI Backend with AI-Powered Document Analysis

[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-green.svg)](https://fastapi.tiangolo.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://www.docker.com/)

**Status:** ğŸš§ MVP - Phase 3 Complete  
**Last Updated:** January 8, 2026

---

## ğŸ“‹ Overview

Production-ready backend system for NGO management with **AI-powered financial document analysis**. Built with FastAPI, PostgreSQL, and OpenAI GPT for intelligent cost/profit extraction from PDF and CSV documents.

### Key Features

**Phase 1: Core CRUD (âœ… Complete)**
- 14 RESTful API endpoints
- 2 database tables (Organizations, Projects)
- SQLAlchemy ORM with relationship management
- Pydantic validation
- Automatic API documentation (Swagger UI)

**Phase 2: File Processing (âœ… Complete)**
- PDF document upload and parsing
- CSV data import and processing
- File type validation and size limits
- Metadata extraction

**Phase 3: AI Document Analysis (âœ… Complete)**
- **Intelligent cost/profit extraction** from documents using OpenAI GPT
- **Structured data extraction**: dates, amounts, vendors, categories
- **Multi-format support**: PDF invoices, receipts, bank statements
- **Confidence scoring** for extracted data
- **Async processing** for optimal performance

---

## ğŸ—ï¸ Architecture

```
Client (Postman/Frontend)
    â†“
FastAPI Backend (app/)
    â”œâ”€â”€ REST API (16 endpoints)
    â”œâ”€â”€ PDF Parser (PyPDF2)
    â”œâ”€â”€ AI Service (OpenAI GPT-4)
    â””â”€â”€ SQLAlchemy ORM
    â†“
PostgreSQL Database
    â”œâ”€â”€ organizations
    â”œâ”€â”€ projects
    â””â”€â”€ documents (with extracted_data JSON)
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker Desktop** (required)
- **Git** (required)
- **OpenAI API Key** (optional, for AI features)

### Installation

```bash
# 1. Clone repository
git clone <your-repo-url>
cd project1

# 2. Configure environment
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY (if using AI features)

# 3. Start services
docker-compose up --build

# âœ… Services will start:
# - PostgreSQL on port 5432
# - Backend API on port 8000
# - Adminer (DB viewer) on port 8090
```

### Verify Installation

```bash
# Check health
curl http://localhost:8000/health
# Response: {"status": "ok"}

# View API documentation
# Open: http://localhost:8000/docs
```

---

## ğŸ“Š API Endpoints

### Organizations (5 endpoints)
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/organizations` | Create organization |
| GET | `/organizations` | List all (paginated) |
| GET | `/organizations/{id}` | Get by ID |
| PUT | `/organizations/{id}` | Update organization |
| DELETE | `/organizations/{id}` | Delete (cascade) |

### Projects (6 endpoints)
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/projects` | Create project |
| GET | `/projects` | List all (paginated) |
| GET | `/projects/{id}` | Get by ID |
| GET | `/organizations/{id}/projects` | Get org's projects |
| PUT | `/projects/{id}` | Update project |
| DELETE | `/projects/{id}` | Delete project |

### Documents (5 endpoints) - **Phase 2 & 3**
| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/documents/upload` | Upload & analyze document (PDF/CSV) |
| GET | `/documents` | List all documents |
| GET | `/documents/{id}` | Get document with extracted data |
| GET | `/documents/{id}/download` | Download original file |
| DELETE | `/documents/{id}` | Delete document |

---

## ğŸ¤– AI-Powered Document Analysis

### How It Works

1. **Upload Document**: POST `/documents/upload` with PDF/CSV file
2. **Automatic Processing**:
   - File validation (type, size)
   - Text extraction (PyPDF2 or pandas)
   - AI analysis via OpenAI GPT
3. **Structured Extraction**:
   ```json
   {
     "type": "cost",
     "date": "2025-12-15",
     "amount": 2500.00,
     "currency": "USD",
     "vendor": "Tech Consulting Inc",
     "category": "Professional Services",
     "confidence": 0.95
   }
   ```

### Example Usage

```bash
# Upload invoice for AI analysis
curl -X POST "http://localhost:8000/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@invoice.pdf"

# Response includes extracted_data with costs/profits
{
  "id": 1,
  "file_name": "invoice.pdf",
  "processing_status": "completed",
  "extracted_data": {
    "type": "cost",
    "amount": 2500.00,
    ...
  }
}
```

### Supported Documents

- **Invoices** (PDF)
- **Receipts** (PDF)
- **Bank Statements** (PDF/CSV)
- **Donation Records** (CSV)
- **Expense Reports** (PDF/CSV)

---

## ğŸ—„ï¸ Database Schema

### Tables

**organizations**
- id (PK)
- name (unique)
- email (unique)
- country, description
- is_active, created_at, updated_at

**projects**
- id (PK)
- name, description
- organization_id (FK)
- status, is_active
- created_at, updated_at

**documents** (Phase 2 & 3)
- id (PK)
- file_name, file_type, file_size
- file_path (internal storage)
- processing_status
- **extracted_data** (JSONB - AI results)
- created_at, updated_at

### Relationships
- 1 Organization â†’ Many Projects (cascade delete)
- Documents independent (can link to orgs in future)

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Framework** | FastAPI | 0.109.0 |
| **Server** | Uvicorn | 0.27.0 |
| **Database** | PostgreSQL | 15 |
| **ORM** | SQLAlchemy | 2.0.23 |
| **Validation** | Pydantic | 2.5.2 |
| **AI Engine** | OpenAI GPT | 4o-mini |
| **PDF Parser** | PyPDF2 | 3.0.1 |
| **Data Processing** | pandas | 2.1.4 |
| **Container** | Docker | Compose v2 |
| **DB Viewer** | Adminer | Latest |

---

## ğŸ“ Project Structure

```
project1/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Settings & env vars
â”‚   â”œâ”€â”€ database.py        # SQLAlchemy setup
â”‚   â”œâ”€â”€ models.py          # ORM models (3 tables)
â”‚   â”œâ”€â”€ schemas.py         # Pydantic validators
â”‚   â”œâ”€â”€ crud.py            # Database operations
â”‚   â”œâ”€â”€ main.py            # FastAPI app & endpoints
â”‚   â”œâ”€â”€ pdf_utils.py       # PDF parsing (Phase 2)
â”‚   â””â”€â”€ ai_service.py      # OpenAI integration (Phase 3)
â”œâ”€â”€ docker-compose.yml     # Multi-container setup
â”œâ”€â”€ Dockerfile            # Python container
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example          # Environment template
â””â”€â”€ README.md             # This file
```

---

## ğŸ§ª Testing

### Interactive API Testing (Swagger UI)

```bash
# 1. Start services
docker-compose up

# 2. Open Swagger UI
# http://localhost:8000/docs

# 3. Test workflow:
# - POST /organizations (create NGO)
# - POST /projects (create project)
# - POST /documents/upload (upload invoice with AI analysis)
# - GET /documents/{id} (view extracted data)
```

### Database Viewer (Adminer)

```bash
# Access live database view
# http://localhost:8090

# Login:
# - System: PostgreSQL
# - Server: postgres
# - Username: ngo_user
# - Password: secure_password
# - Database: ngo_db

# View real-time data without SQL!
```

### Manual Testing

```bash
# Health check
curl http://localhost:8000/health

# Create organization
curl -X POST "http://localhost:8000/organizations" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Green Earth NGO",
    "email": "contact@greenearth.org",
    "country": "Germany"
  }'

# Upload document for AI analysis
curl -X POST "http://localhost:8000/documents/upload" \
  -F "file=@invoice.pdf"
```

---

## ğŸ” Security Best Practices

âœ… **Implemented:**
- Environment variables for secrets (.env not in Git)
- Pydantic validation on all inputs
- SQLAlchemy ORM (SQL injection protection)
- File type validation (only PDF/CSV)
- File size limits (10MB default)
- CORS middleware configured
- Database connection pooling

âš ï¸ **Production TODO:**
- Add authentication (JWT tokens)
- Add rate limiting
- Enable HTTPS
- Add user roles/permissions
- Implement audit logging
- Set up monitoring (Sentry/DataDog)

---

## ğŸš§ Development Status

| Phase | Feature | Status |
|-------|---------|--------|
| **Phase 1** | Core CRUD API | âœ… Complete |
| **Phase 2** | File Upload/Processing | âœ… Complete |
| **Phase 3** | AI Document Analysis | âœ… Complete |
| **Phase 4** | Authentication/Authorization | ğŸ“‹ Planned |
| **Phase 5** | Multi-tenant Support | ğŸ“‹ Planned |
| **Phase 6** | Production Deployment | ğŸ“‹ Planned |


---

## ğŸ› ï¸ Troubleshooting

### Port Already in Use
```bash
# Stop containers
docker-compose down

# Check ports
lsof -i :8000  # Backend
lsof -i :5432  # PostgreSQL
lsof -i :8090  # Adminer

# Kill process or change ports in docker-compose.yml
```

### Database Connection Failed
```bash
# Check postgres container
docker logs ngo_postgres

# Restart services
docker-compose restart postgres
docker-compose restart backend
```

### AI Features Not Working
```bash
# 1. Verify OpenAI API key in .env
echo $OPENAI_API_KEY  # Should not be empty

# 2. Check backend logs
docker logs ngo_backend

# 3. Test API key
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### View Logs
```bash
# All services
docker-compose logs -f

# Specific service
docker logs ngo_backend
docker logs ngo_postgres
docker logs ngo_adminer
```

---

## ğŸ¤ Contributing

This is a NGO Automation System MVP project. For suggestions or issues:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add feature'`)
4. Push to branch (`git push origin feature/improvement`)
5. Open Pull Request

---

## ğŸ“„ License

Educational project - [Master School](https://masterschool.com)

---

## ğŸ‘¤ Author

**Student Project**  
Master School - Data Science & AI Program  
January 2026

---

## ğŸ¯ Next Steps

### For Development
1. Add authentication (JWT)
2. Implement user roles
3. Add document categorization
4. Create financial dashboard
5. Add export to Excel/PDF

### For Production
1. Set up CI/CD pipeline
2. Configure production database
3. Add monitoring/logging
4. Implement backup strategy
5. Deploy to cloud

---

## âš¡ Performance

- **API Response Time**: <100ms (average)
- **Document Processing**: 2-5 seconds (depends on file size)
- **AI Extraction**: 3-8 seconds (OpenAI API latency)
- **Concurrent Requests**: Supports async processing

---

## ğŸ“ Support

For questions about this project:
- Check Swagger UI: http://localhost:8000/docs
- View database: http://localhost:8090 (Adminer)

---

**Built with â¤ï¸ using FastAPI, PostgreSQL, and OpenAI**
